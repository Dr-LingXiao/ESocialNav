import os
import torch

from ..utils import *
from ..model import *

class BaseTrainingRecipe:

    def __init__(self, training_arguments):
        self.training_arguments = training_arguments

    def __call__(self, model):
        model = self.training_model_converse(model)
        model = self.tune_type_setting(model)
        model.config.tune_type_connector = self.training_arguments.tune_type_connector
        model.config.tune_type_vision_tower = self.training_arguments.tune_type_vision_tower
        model.config.tune_type_llm = self.training_arguments.tune_type_llm
        model.config.tune_vision_tower_from_layer = self.training_arguments.tune_vision_tower_from_layer
        return model

    def add_args(self, model_args):
        llm_dtype = (torch.float16 if self.training_arguments.fp16 else (torch.bfloat16 if self.training_arguments.bf16 else torch.float32))
        model_args['llm'].update(dict(torch_dtype=llm_dtype))
        if self.training_arguments.pretrained_model_path is not None:
            model_args['llm'].update(dict(pretrained_llm_path=os.path.join(self.training_arguments.pretrained_model_path, 'language_model')))
            model_args['vision_tower'].update(dict(pretrained_vision_tower_path=os.path.join(self.training_arguments.pretrained_model_path, 'vision_tower')))
            model_args['connector'].update(dict(pretrained_connector_path=os.path.join(self.training_arguments.pretrained_model_path, 'connector')))
        return model_args

    def tune_type_setting(self, model):
        model = self._llm_tune_type_setting(model)
        model = self._vision_tower_tune_type_setting(model)
        model = self._connector_tune_type_setting(model)
        return model

    # def _llm_tune_type_setting(self, model):
    #     tune_type = self.training_arguments.tune_type_llm.lower()
    #     assert tune_type in ('frozen', 'full', 'lora', 'qlora'), \
    #         f'tune_type {tune_type} not supported in this training recipe!'

    #     if tune_type == 'full':
    #         # å…¨å‚æ•°è®­ç»ƒï¼šLLM å…¨éƒ¨å¯è®­ç»ƒ
    #         model.language_model.requires_grad_(True)
    #     elif tune_type == 'frozen':
    #         # å®Œå…¨å†»ç»“ LLM
    #         model.language_model.requires_grad_(False)
    #     # ğŸ”´ lora / qloraï¼šè¿™é‡Œä¸è¦å†åŠ¨ requires_grad
    #     #    è®© LoRATrainingRecipe / QLoRATrainingRecipe åœ¨ training_model_converse é‡Œè‡ªå·±æ§åˆ¶

    #     # ä¿ç•™åŸæ¥çš„ gradient checkpointing æ”¯æŒ
    #     self.support_gradient_checkpoint(model.language_model, self.training_arguments.gradient_checkpointing)
    #     return model

    def _llm_tune_type_setting(self, model):
        tune_type = self.training_arguments.tune_type_llm.lower()
        assert tune_type in ('frozen', 'full', 'lora', 'qlora'), f'tune_type {tune_type} not supported in this training recipe!'
        if tune_type == 'full':
            model.language_model.requires_grad_(True)
        elif tune_type == 'frozen':
            model.language_model.requires_grad_(False)
        self.support_gradient_checkpoint(model.language_model, self.training_arguments.gradient_checkpointing)
        return model

    def _vision_tower_tune_type_setting(self, model):
        tune_type = self.training_arguments.tune_type_vision_tower.lower()
        assert tune_type in ('frozen', 'full', 'partially-tune', 'lora', 'qlora'), f'tune_type {tune_type} not supported in this training recipe!'
        if tune_type == 'full':
            model.vision_tower.requires_grad_(True)
        elif tune_type == 'frozen':
            model.vision_tower.requires_grad_(False)
        elif tune_type == 'partially-tune':
            from_layer = self.training_arguments.tune_vision_tower_from_layer
            if from_layer > -1:
                log(f'Tune the vision tower from layer {from_layer}!')
                for n, p in model.vision_tower.named_parameters():
                    if 'vision_model.encoder.layers.' in n:
                        layer_id = int(n.split('vision_model.encoder.layers.')[-1].split('.')[0])
                        p.requires_grad = (layer_id >= from_layer)
                    else:
                        p.requires_grad = False
        return model

    def _connector_tune_type_setting(self, model):
        tune_type = self.training_arguments.tune_type_connector.lower()
        assert tune_type in ('frozen', 'full', 'lora', 'qlora'), f'tune_type {tune_type} not supported in this training recipe!'
        if tune_type == 'full':
            for p in model.connector.parameters():
                p.requires_grad = True
        elif tune_type == 'frozen':
            for p in model.connector.parameters():
                p.requires_grad = False
        return model

    def training_model_converse(self, model):
        return model

    def save(self, model, trainer):
        model.config.use_cache = True
        # save tokenizer
        model.tokenizer.save_pretrained(self.training_arguments.output_dir)
        # save entire model config
        model.config.save_pretrained(self.training_arguments.output_dir, from_pt=True)
        # save trainer
        trainer.save_state()

        # finetune é˜¶æ®µï¼šæ•´åŒ…ä¿å­˜ï¼ˆæ˜¯å¦ merge ç”± lora_recipe æ§åˆ¶ï¼‰
        if 'finetune' in self.training_arguments.output_dir and self.training_arguments.pretrained_model_path is not None:
            if trainer.deepspeed:
                torch.cuda.synchronize()
            trainer.save_model(self.training_arguments.output_dir)
            return
        
        # if 'finetune' in self.training_arguments.output_dir and self.training_arguments.pretrained_model_path is not None:
        #     if trainer.deepspeed:
        #         torch.cuda.synchronize()
        #     trainer.save_model(self.training_arguments.output_dir)
        #     return

        # pretrain é˜¶æ®µåˆ†åˆ«ä¿å­˜å„å­æ¨¡å—ï¼ˆé LoRAï¼‰
        language_model_state_dict = get_state_maybe_zero_3(model.language_model.named_parameters(), [''], False)
        if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:
            language_model_output_dir = os.path.join(self.training_arguments.output_dir, 'language_model')
            os.makedirs(language_model_output_dir, exist_ok=True)
            language_model_output_path = os.path.join(self.training_arguments.output_dir, 'language_model/pytorch_model.bin')
            torch.save(language_model_state_dict, language_model_output_path)
            model.config.text_config.save_pretrained(language_model_output_dir, from_pt=True)

        vision_tower_state_dict = get_state_maybe_zero_3(model.vision_tower._vision_tower.named_parameters(), [''], False)
        if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:
            vision_tower_output_dir = os.path.join(self.training_arguments.output_dir, 'vision_tower')
            os.makedirs(vision_tower_output_dir, exist_ok=True)
            vision_tower_output_path = os.path.join(self.training_arguments.output_dir, 'vision_tower/pytorch_model.bin')
            torch.save(vision_tower_state_dict, vision_tower_output_path)
            if isinstance(model.vision_tower._vision_tower, PreTrainedModel):
                model.vision_tower._vision_tower.config.save_pretrained(vision_tower_output_dir, from_pt=True)

        connector_state_dict = get_state_maybe_zero_3(model.connector.named_parameters(), [''], False)
        if trainer.args.local_rank == 0 or trainer.args.local_rank == -1:
            connector_output_dir = os.path.join(self.training_arguments.output_dir, 'connector')
            os.makedirs(connector_output_dir, exist_ok=True)
            connector_output_path = os.path.join(self.training_arguments.output_dir, 'connector/pytorch_model.bin')
            torch.save(connector_state_dict, connector_output_path)

    def load(self, model, model_args={}):
        # æ—¢æ”¯æŒæ ¹ç›®å½•ï¼Œä¹Ÿæ”¯æŒ lora_adapter/ å­ç›®å½•
        ckpt_root = self.training_arguments.pretrained_model_path
        has_root_adapter = (ckpt_root is not None) and os.path.exists(os.path.join(ckpt_root, 'adapter_config.json'))
        has_sub_adapter  = (ckpt_root is not None) and os.path.exists(os.path.join(ckpt_root, 'lora_adapter', 'adapter_config.json'))
        is_lora_ckpt = has_root_adapter or has_sub_adapter

        if not is_lora_ckpt:
            # é LoRAï¼šä¿æŒåŸé€»è¾‘ï¼Œä¸å½±å“é lora pretrain
            model.load_llm(**model_args['llm'])
            model.load_vision_tower(**model_args['vision_tower'])
            model.load_connector(**model_args['connector'])
            return model

        # LoRA æƒ…å†µï¼šä¼˜å…ˆä½¿ç”¨ä½ ä¿å­˜ä¸‹æ¥çš„ baseï¼ˆlanguage_model/ï¼‰ï¼›æ²¡æœ‰å°±å›é€€åˆ° hub æ¨¡å‹
        base_llm_src = model_args['llm'].get('pretrained_llm_path') or model_args['llm']['model_name_or_path']
        model.language_model = model.language_model.from_pretrained(
            base_llm_src,
            attn_implementation='flash_attention_2',
            torch_dtype=model_args['llm']['torch_dtype']
        )
        model.load_vision_tower(**model_args['vision_tower'])
        model.load_connector(**model_args['connector'])
        model.to(model_args['llm']['torch_dtype'])

        from peft import PeftModel
        adapter_dir = os.path.join(ckpt_root, 'lora_adapter') if has_sub_adapter else ckpt_root

        # å…³é”®ï¼šç¬¬äºŒé˜¶æ®µç»§ç»­ LoRA è®­ç»ƒ â†’ ä¸åˆå¹¶ï¼Œç›´æ¥ä»¥å¯è®­ç»ƒæ–¹å¼æŒ‚ä¸ŠåŒä¸€ä¸ª adapter
        train_with_lora = self.training_arguments.tune_type_llm.lower() in ('lora', 'qlora')
        print('Loading LoRA adapter from:', adapter_dir)
        model = PeftModel.from_pretrained(
            model,
            adapter_dir,
            is_trainable=train_with_lora,
            adapter_name="default"
        )

        if train_with_lora:
            print('LoRA attached for training (no merge).')
        else:
            print('Merging LoRA weights for non-LoRA training/inference...')
            model = model.merge_and_unload()
            print('Merged.')

        print('Model is loaded...')
        return model

    # def load(self, model, model_args={}):
    #     # å½“é¢„è®­ç»ƒè·¯å¾„ä¸æ˜¯ LoRA å½¢å¼æ—¶ï¼ŒæŒ‰å¸¸è§„åŠ è½½
    #     is_lora_ckpt = (
    #         self.training_arguments.pretrained_model_path is not None
    #         and os.path.exists(os.path.join(self.training_arguments.pretrained_model_path, 'adapter_config.json'))
    #     )
    #     if not is_lora_ckpt:
    #         model.load_llm(**model_args['llm'])
    #         model.load_vision_tower(**model_args['vision_tower'])
    #         model.load_connector(**model_args['connector'])
    #         return model

    #     # LoRA æƒé‡ï¼šé»˜è®¤å¤ç”¨ï¼Œä¸åˆå¹¶ï¼Œä¾¿äºç»§ç»­è®­ç»ƒ
    #     model.language_model = model.language_model.from_pretrained(
    #         model_args['llm']['model_name_or_path'],
    #         attn_implementation='flash_attention_2',
    #         torch_dtype=model_args['llm']['torch_dtype']
    #     )
    #     model.load_vision_tower(**model_args['vision_tower'])
    #     model.load_connector(**model_args['connector'])
    #     model.to(model_args['llm']['torch_dtype'])

    #     from peft import PeftModel
    #     print('Loading LoRA weights...')
    #     model = PeftModel.from_pretrained(model, self.training_arguments.pretrained_model_path)

    #     reuse_lora = getattr(self.training_arguments, "reuse_lora", False)
    #     if reuse_lora:
    #         print('Keeping LoRA adapters attached (no merge) for reuse...')
    #     else:
    #         print('Merging LoRA weights (reuse_lora=False)...')
    #         model = model.merge_and_unload()
    #     print('Model is loaded...')
    #     return model

    def support_gradient_checkpoint(self, model, gradient_checkpointing=False):
        def make_inputs_require_grad(module, input, output):
            output.requires_grad_(True)
        if gradient_checkpointing:
            if hasattr(model, "enable_input_require_grads"):
                model.enable_input_require_grads()
            else:
                model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)
